mode: train
data:
  train_paths: ['data/libri_train_1h/data_list_sorted.json',
                'data/libri_train_9h/data_list_sorted.json']
  dev_paths: ['data/libri_dev/data_list_sorted.json']
  text:
    mode: character
    vocab: data/libri_train_1h/vocab_char.txt

model:
  name: transformer_trans
  feature_dim: 240
  extractor:
    name: fbank
    train: false
    feature: hidden_states
  dropout: 0.2
  enc:
    conformer:
      dim: 128
      ff_mult: 4
      conv_expansion_factor: 2
      conv_kernel_size: 31
      attn_dropout: 0.2
      ff_dropout: 0.2
      conv_dropout: 0.0
    hidden_size: 128
    output_size: 128
    n_layers: 2
    
    transformer:
      d_model: 128
      nhead: 8
      dim_feedforward: 512
      dropout: 0.2
      activation: "relu"
      layer_norm_eps: 0.00001
      batch_first: False
      norm_first: False

  joint:
    input_size: 256
    inner_size: 128
  dec:
    hidden_size: 128
    output_size: 128
    n_layers: 1
  share_weight: False
  share_embedding: False
  optim:
    algo: Adam
    kwargs:
      lr: 0.0005
  specaugment:
    freq_mask_range: [0, 20]
    freq_mask_num: 2
    time_mask_range: [0, 40]
    time_mask_num: 2
    time_mask_max: 1.0
    time_warp_w: 80

hparam:
  train_batch_size: 8
  val_batch_size: 8
  accum_grad: 4
  grad_clip: 5
  njobs: 4
  pin_memory: true

checkpoint_callbacks:
  monitor: val_wer
  mode: min
  save_top_k: 5  # -1: all ckpts will be saved

trainer:
  max_epochs: 500
  max_steps: 100000
  check_val_every_n_epoch: 5
  gpus: 1
  precision: 16
  logger: true
  log_every_n_steps: 5
  flush_logs_every_n_steps: 5
  default_root_dir: model/transformer_trans
  deterministic: true
